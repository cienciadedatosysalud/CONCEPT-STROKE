---
title: "CONCEPT STROKE: Analytical pipeline"
author: Data Science for Health Services and Policy Research (IACS)
editor: source
#date: 
# bibliography: 
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 4
    highlight-style: pygments
    code-fold: true
    html-math-method: katex
execute: 
  warning: false
  cache: false
  echo: false
params:
    data_path: '../../inputs/data.duckdb'
---

```{css, echo = FALSE}
.justify {
  text-align: justify !important
}
```



## Introduction

### Definition

CONCEPT STROKE is a study analysing the acute care received by patients with acute ischaemic stroke where the aim is to show the relevance of care pathways on outcomes (traces/trajectories) and efficiency of stroke care.

-   Participating regions: Aragón, País Vasco, Cataluña, Navarra and Valencia.

It is a two-stage design: 

1- Cross-sectional data mining design 

2- Quasi-experimental design comparing interventions in acute ischaemic stroke.

The Main endpoints are: 

1- In the first stage, the pathway of care as it occurs in real life and the propensity of a patient to follow a specific pathway (trace). 

2- In the second stage, the survival of patients 30 days and 6 months after the admission to an emergency room

### Cohort

The cohort is defined as patients admitted to hospital due to acute ischaemic stroke.

-   Inclusion criteria: Patients aged 18 years or older admitted to the emergency department (or with an unplanned hospital admission) with a principal diagnosis of acute ischaemic stroke during the study period.

-   Exclusion criteria: Patients aged 17 years or younger; Patients with a diagnosis of acute haemorrhagic stroke or with other non-specific stroke diagnoses.

-   Study period: 01-01-2010 to 31-12-2022.

## Analysis plan


```{r}
# Sys.setenv(RETICULATE_PYTHON = "/opt/conda/envs/aspire/bin/python")
# reticulate::use_python("/opt/conda/envs/aspire/bin/python")
# options(reticulate.conda_binary = path.expand("/bin/micromamba"))
# reticulate::use_condaenv(condaenv="/opt/conda/envs/aspire", conda="/opt/conda/envs/aspire/conda")
```


```{python}
#| label: Load packages python

# # new import
# import random
# import pm4py, datetime, subprocess
# import pandas as pd
# import numpy as np
# import duckdb
# #from pygam import PoissonGAM, LinearGAM, s, f, te, l
# # from sksurv.nonparametric import kaplan_meier_estimator
# import matplotlib.pyplot as plt
# from lifelines import KaplanMeierFitter, CoxPHFitter
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import RepeatedStratifiedKFold
# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# from pm4py.objects.conversion.log import converter as log_converter
# 
# # process mining
# from pm4py import PetriNet
# from pm4py.algo.discovery.alpha import algorithm as alpha_miner
# from pm4py.algo.discovery.inductive import algorithm as inductive_miner
# from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
# from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
# 
# # viz
# from pm4py.visualization.petri_net import visualizer as pn_visualizer
# from pm4py.visualization.process_tree import visualizer as pt_visualizer
# from pm4py.visualization.heuristics_net import visualizer as hn_visualizer
# from pm4py.visualization.dfg import visualizer as dfg_visualization
# 
# # misc
# from pm4py.visualization.petri_net import visualizer
# from pm4py.algo.decision_mining import algorithm as decision_mining


```


```{r}
#| label: load r packages
library(duckdb)
library(dplyr)
library(knitr)
library(Hmisc)
library(mgcv)
library(lubridate)
library(bupaverse)
library(purrr)
library(processpredictR)
library(keras)
library(tensorflow)
### new library
library(survival)
library(survminer)
library(stringr)
library(timeDate)
library(sjPlot)
library(forestmodel)
library(plotly)
library(logger)
library(gt)
library(reshape2)
```



### Descriptive analysis

To study the observed data, we performed a small exploratory analysis of the data. First, we built a descriptive table of the imported data that shows the main characteristics of the patients registered in the database. After, we must convert our DataFrame data to an Event Log object. However, one of the drawbacks we may have when creating our Event Log is the granularity of the dates, as hospital dates are usually accurate to the day while emergency dates are usually accurate to the second. Therefore, we need to generate a function to check that they are correct and see if any of them do not make medical sense. We may find errors such as, for example, having emergency and hospital dates on the same day and, as they have different granularity, automatically the hospital date is ordered first or, for example, emergency discharge date is prior to the admission date, among others.  

As part of the exploratory analysis it is interesting to know how many different pathways appear and the frequency of each one. This allows us to know what percentage of the pathways are among the most frequent pathways and which pathways are isolated cases. 

#### Survival analysis

We carried out a survival analysis with the 10 most frequent traces, constructing a Kaplan-Meier curve for each of them for comparison. Subsequently, a COX model was performed to compare these pathways and observe the Hazard Ratio (HR) of each pathway compared to the rest.

### Process Mining

For process mining we created several functions depending on the part of the process mining study, which can be divided into: 

- Process discovery 

- Conformance checking 

- Decision mining 

- Prediction

#### Process discovery

Process discovery attempts to find a suitable process model that describes the order of events/activities that are executed during the execution of a process.

The next step after the descriptive analysis was to build a Petri net to discover the process. For this, there are different algorithms: alpha mining, inductive mining or heuristic mining.

However, another type of graph that can be built is the Directly Follows Graph (DFG), which is a graph that, although it can be part of the discovery process, serves as a descriptive analysis as it shows all possible pathways present in the data.

To reduce the high dimensionality, one option is to filter the traces to keep only the k most frequent traces. In this case we filtered by the k=10 most frequent traces.

#### Conformance checking

Conformance checking is a technique for comparing a process model with an event record of the same process. However, as a first approximation, we made a comparison between the most frequent pathways and the one we established as theoretical by Jaccard similarity without taking into account the order, so that the quotient between is calculated:

-   Numerator: number of activities that coincide between each of these pathways with the standard.

-   Denominator: number of activities of the union between each one of these pathways with the regulation.


#### Decision mining

Decision mining allows us to know what are the main characteristics of patients that make them follow a certain path. To do this, patient characteristics are added to the Event log. Petri net is created using the inductive algorithm and the decision points of the net are observed. We see the importance of the characteristics at one decision point. It is like a decision tree at the decision point. This step may be helpful to know which variables are of importance for input into the prediction model in the next section.

#### Prediction

To predict which pathway a given patient should follow based on his or her characteristics, we have used the *bupaR* library, which makes use of a transformer model to predict the pathway as a sequence of activities. Thus, an Event log with features and a transformer model is used to predict the next activity.


### Estimation of outcomes within a path

A proportional-hazard regression with the scores of propensity of specific paths being the main independent factor in the prediction of survival at 6 and 12 months after admission.

#### Kaplan-Meier survival plot

The kaplan-meier survival plot for the 4 intervention possibilities is shown below:

- None

- Fibrinolysis

- Thrombectomy mechanic

- Combined (fibrinolyisis + thrombectomy mechanic)


#### General COX model

A COX model is built with the survival object (time = survival time; case = exitus) as the dependent variable and the following variables as independent variables: 

- Categorical: intervention, sex, zip code, hospital, hospital type dicharge, weekday, modified rankin scale, rank trace, holidays, weekend, prescriptions and comorbidities,

- Numerical: age, jaccard similarity measure, duration trace, period, number of admission prior emergency, number of admission prior inhospital.

#### Propension to intervention model

In order to overcome the violation of proportional risk in COX model, 4 different propensity to intervention models are estimated, using as covariables those that have been found to be significant (*pvalue* < 0.05) in the general COX model constructed in the previous section.

1. Propensity to fibrinolysis intervention model,

2. Propensity to thrombectomy mechanic intervention model,

3. Propensity to combined intervention model,

4. Propensity to any intervention model


After building the models, the propensity to intervention for each patient is predicted using each model and the propensity score (PS) is calculated according to the formula: $$PS_i = 1/ (1- p(y)_i)$$ 
where *p(y)* indicates the prediction for each patient *i*.


#### Model to predict exitus with PS as covariable

After calculating the PS for each intervention, a model is constructed for each intervention to predict exitus with the PS of this corresponding intervention as a covariate and finally a general model with the PS of each intervention as covariates and the PS of any intervention as an offset.

## Results

These results have been carried out with synthetic data previously generated according to the data model. 

### Descriptive analysis

First, we built a descriptive table of the imported data that shows the main characteristics of the patients registered in the database.



```{r, echo=FALSE,warning=FALSE, output = FALSE}
#| label: query descriptive


con = dbConnect(duckdb::duckdb(), dbdir=params$data_path, read_only=FALSE)

descriptive_values <- dbGetQuery(conn = con, "
SELECT median(age_nm) as median_age, 
(QUANTILE_CONT(age_nm, 0.75)-QUANTILE_CONT(age_nm, 0.25)) as iqr_age,
count(distinct patient_id) filter (where sex_cd = '1') as men,
count(distinct patient_id) filter (where sex_cd = '2') as women,
count(distinct patient_id) as n_patient_distinct,
count(distinct patient_id) filter (where inhospital_fibrinolysis_bl = TRUE OR thrombolysis_emergency_dt is NOT NULL) as n_patient_with_fibrinolysis,
count(distinct patient_id) filter (where inhospital_fibrinolysis_bl = TRUE) as n_patient_with_thrombectomy,
count(distinct patient_id) filter (where exitus_dt is NOT NULL) as n_patient_exitus,
FROM main.patient")

descriptive_values <- descriptive_values %>% mutate(
  perc_men = round((men/n_patient_distinct)*100,2),
  united_men =paste0(men, ' (',perc_men,'%)' ),
  perc_women = round((women/n_patient_distinct)*100,2),
  united_women =paste0(women, ' (',perc_women,'%)' )
)

descriptive_values <- descriptive_values %>% select(!c(men,women,perc_men,perc_women))



comorbidities <- dbGetQuery(conn=con,"
with seleccion as (
SELECT
	patient_id ,
	list_aggregate([heart_failure_bl::int,
  hypertension_bl::int,
	diabetes_bl::int,
  atrial_fibrillation_bl::int,
	valvular_disease_bl::int],
	'sum') as sum_comorbidities
from
	main.patient)
select
	sum_comorbidities,
	count(DISTINCT patient_id) as patient_distinct_n_comorbidities
from
	seleccion
group by
	sum_comorbidities")

comorbidities_ <- data.frame(comor_0 = sum(filter(comorbidities, sum_comorbidities == 0)$patient_distinct_n_comorbidities),
                             comor_1_2 = sum(filter(comorbidities, sum_comorbidities > 0 & sum_comorbidities <=2)$patient_distinct_n_comorbidities),
                             comor_2mayor = sum(filter(comorbidities,sum_comorbidities >2)$patient_distinct_n_comorbidities))
comorbidities_ <- melt(comorbidities_)
comorbidities_ <- comorbidities_ %>% mutate(
  perc_comorbidities = round((value/descriptive_values$n_patient_distinct)*100,2),
  united_comorbidities=paste0(value, ' (',perc_comorbidities,'%)' )
)
rm(comorbidities)

dbDisconnect(con, shutdown=TRUE)

```


```{r, echo=FALSE,warning=FALSE}
#| label: descriptive tables inputs1

descriptive_values_ <- melt(descriptive_values,id=NULL) 

comorbidities_ <- comorbidities_ %>% arrange(variable) %>% dplyr::select(variable,united_comorbidities) %>% 
  mutate(descrp = case_when(
    variable %in% 'comor_0' ~ '0 N comorbidities',
    variable %in% 'comor_1_2' ~ '1 <= N comorbidities <= 2',
    variable %in% 'comor_2mayor' ~ '2 < N comorbidities',
    TRUE ~ 'Unknown'))

descriptive_values_$variable <- as.character(descriptive_values_$variable)


descriptive_values_$variable[descriptive_values_$variable %in% 'median_age'] <- 'Median age'
descriptive_values_$variable[descriptive_values_$variable %in% 'iqr_age'] <- 'IQR age'
descriptive_values_$variable[descriptive_values_$variable %in% 'n_patient_distinct'] <- 'N patients (unique)'
descriptive_values_$variable[descriptive_values_$variable %in% 'n_patient_with_fibrinolysis'] <- 'N patients (unique) with fibrinolysis'
descriptive_values_$variable[descriptive_values_$variable %in% 'n_patient_with_thrombectomy'] <- 'N patients (unique) with thrombectomy'
descriptive_values_$variable[descriptive_values_$variable %in% 'n_patient_exitus'] <- 'N patient with exitus (unique) date (death)'
descriptive_values_$variable[descriptive_values_$variable %in% 'united_men'] <- 'N Men (%)'
descriptive_values_$variable[descriptive_values_$variable %in% 'united_women'] <- 'N Women (%)'


table <- descriptive_values_ %>% gt(rowname_col = 'variable') %>% 
  tab_header(
    title = "Patient-level summary"
  ) %>% 
  tab_stubhead(label = "Variable") %>% 
     cols_label(
      variable = 'Variable',
      value = 'n (%)'
       ) %>% 
  cols_align(
  align =  "center",
  columns = value)

table

comorbidities_ %>% 
  dplyr::select(descrp, united_comorbidities) %>% gt(rowname_col = 'descrp') %>% 
  tab_header(
    title = "Number of comorbidities table input"
  ) %>%
     cols_label(
      descrp = 'Number of comorbidities',
      united_comorbidities = 'n (%)'
       ) %>% 
  cols_align(
  align =  "center",
  columns = united_comorbidities)
```

Following, we imported and converted our DataFrame to an Event Log and check dates. 


```{r,echo=FALSE}
log_info("Analyses start")
```


```{python, echo = FALSE, warning = FALSE}
#| label: main code
import logging
logging.basicConfig(level=logging.INFO)


from aux_scripts.create_event_log import import_data_and_convert_to_event_log, check_dates_hospital_emergency, filter_for_k_freq_traces
data_path = '../../inputs/data.duckdb'
trace_theory = str('admission_emergency_care_dt,triage_emergency_care_dt,first_asisstance_medical_dt,internal_neurology_consultation_dt,ct_mri_dt,admission_to_observation_ward_dt,thrombolysis_emergency_dt,discharge_from_emergency_dt')

#logging.info('Read database and create eventlog')
event_log, traces, freq_traces, df = import_data_and_convert_to_event_log(data_path)


check_dates_hospital_emergency(data_path)

#logging.info('Extract only 10 most frequent traces and their event log')
filtered_event_log, df_filtered, traces_filtered = filter_for_k_freq_traces(event_log, traces, freq_traces, df, k=10)

```


As a descriptive measure of the data, a bar plot with the number of distinct traces and their frequency.
```{r, echo = FALSE, warning = FALSE, out.width = "1820px", out.height = "720px" , fig.cap="Bar plot with the number of distinct traces and their frequency"}
#| label: fig-bar_plot_frequency

include_graphics('../../outputs/barplot_unique_traces.png')
```

#### Survival analysis

The kaplan-meier curves for the 10 most common pathways and the COX model summary are shown.

```{python, echo = FALSE, warning = FALSE, fig.cap="Survival analysis"}
#| label: survival analysis results

from aux_scripts.survival_descriptive import survival_analysis

survival_analysis(df_filtered)

```



```{r, echo = FALSE, warning = FALSE, out.width = "620px", out.height = "520px", fig.cap="Survival descriptive"}
#| label: fig-survival_descriptive

include_graphics('../../outputs/surv_analysis.png')
```


### Process mining

#### Process discovery


Inductive miner traces:

```{python, echo = FALSE, warning = FALSE, output = FALSE}
#| label: Inductive miner results

from aux_scripts.inductive_miner import inductive_miner_algorithm, get_decision_mining

#inductive_miner_algorithm(filtered_event_log)
logging.info('Get decision mining points')
try:
    point_decision = get_decision_mining(df, event_log)
except:
    logging.basicConfig(level=logging.INFO)
    logging.info('Error in function: Get decision mining points')


```

```{r, echo = FALSE, warning = FALSE, out.width = "2920px",  out.height = "680px", fig.cap="Inductive miner with only filtered traces"}
#| label: fig-Inductive_miner

tryCatch(
  {
  include_graphics('../../outputs/inductive_miner_petri_net.png')
  },
  error=function(cond) {
    message(paste0("error building plot: inductive miner petri net."))
    print(paste0("error building plot: inductive miner petri net."))
  }
) 

```



#### Conformance checking

Comparison of the 10 most frequent traces with the theoretical one, using Jaccard's similarity method without taking into account the order:

The theorical trace is:

```{python, echo = FALSE, warning = FALSE}
#| label: theorical trace 

from aux_scripts.create_df_estimation_outcomes_df import df_model_estimation_outcomes, jaccard_similarity

#print(trace_theory)

df_model_estimation_outcomes(data_path, df, event_log, traces, freq_traces, trace_theory)

```

```{r, echo = FALSE, warning = FALSE,out.width = "1220px", out.height = "320px"}
#| label: print theorical trace 

### ficticial timestamp, patient_id to print theorical trace 
init <- ymd_hms('2012-01-04 10:27:02')
trace_theory_df <- data.frame(activity =c('admission emergency care dt','triage emergency care dt','first asisstance medical dt','internal neurology consultation dt','ct or mri dt','admission to observation ward dt','thrombolysis emergency dt','discharge from emergency dt'))
trace_theory_df$patient_id <- 'patient_trace_theory'
trace_theory_df$activity_instance <- 1:nrow(trace_theory_df)
trace_theory_df$registration_type <- 'completed'
trace_theory_df$resource_id <- 'employee'
trace_theory_df$timestamp <- seq(init,init + 60*60*(nrow(trace_theory_df)-1), by='hour')
p <- trace_theory_df %>% eventlog(case_id = 'patient_id',
              activity_id = 'activity',
              activity_instance_id = 'activity_instance',
              timestamp = 'timestamp',
              lifecycle_id = 'registration_type',
              resource_id = 'resource_id')

p %>% process_map()

rm(init,trace_theory_df,p)
```


The Jaccard's similarity measured for k=10 most frequent traces:

```{r, echo = FALSE, warning = FALSE}
#| label: jaccard similarity 10mostfrequent
con = dbConnect(duckdb::duckdb(), dbdir=params$data_path, read_only=FALSE)

df <- dbGetQuery(conn = con, 'with seleccion_top10 as (SELECT
		*
	FROM
		(
		SELECT
			a.*,
			b.*
		FROM
			main.event_log a
		LEFT JOIN main.prediction_outcomes_db b ON
			a."case:concept:name" = b.patient_id) a
	WHERE
		rank_trace != \'otros\'
		and rank_trace::int <= 10)
select * from seleccion_top10 where "case:concept:name" in (select DISTINCT on(rank_trace) "case:concept:name" from seleccion_top10)')
dbDisconnect(con, shutdown=TRUE)


df <- df %>% rename(activity = `concept:name`, timestamp = `time:timestamp`) %>% 
  select(!ends_with("_dt"))


p <- eventlog(df, case_id='patient_id',
              activity_id = 'activity',
              activity_instance_id = 'activity_instance',
              timestamp = 'timestamp',
              lifecycle_id = 'registration_type',
              resource_id = 'resource_id')
# df <- prepare_examples(p, task = "next_activity",
#                        features = c('hospital_cd','ccaa_cd', 'healthcare_area_cd', 
#                                     'age_nm', 'sex_cd', 'socioeconomic_level_cd','municipality_code_cd','type_admission_cd','heart_failure_bl','atc_code_antiaggregants_cd', 'barthel_index_nm', 'intervention_bl','intervention'))


plot <- p %>%
    trace_explorer(n_traces = 10, plotly = FALSE,coverage_labels = c('cumulative'),label_size = 0)

plot[["data"]][["facets_cum"]] <- round(plot[["data"]][["jaccard_similarity"]],2)

plot + labs(title='Jaccard similarity measured (10 most frequent traces)') + 
  theme(title = element_text(size = 10))
rm(df,p,plot)

```




```{python, echo = FALSE, warning = FALSE}
#| label: comparison checking results

# SequenceMatcher(None, traces_filtered['trace'][0], trace_theory).ratio()
# for i in range(len(traces_filtered['trace'].unique())):
#     p = round(jaccard_similarity(traces_filtered['trace'].unique()[i], trace_theory),2)
#     print(p)

```

Also shown below is the histogram of the Jaccard similarity of the patient traces compared to the theoretical one

```{r, echo = FALSE, warning = FALSE, out.width = "680px",  out.height = "520px", fig.cap="Petri net wiht points for decision mining"}
#| label: fig-hist_jaccard

include_graphics('../../outputs/histogram_jaccard_similarity.png')
```



#### Decision mining

First we created a petri net with the inductive algorithm that will allow us to know the different decision points, we can see that the two images below are the same petri net, showing in the first one the decision points and in the second one the activities (transitions). In this section, in order to create a petri net with decision points, it was necessary to delete records from the event log.


```{r, echo = FALSE, warning = FALSE, out.width = "2920px",  out.height = "680px", fig.cap="Petri net wiht points for decision mining"}
#| label: fig-decision_mining_petri

tryCatch(
  {
  include_graphics('../../outputs/decision_mining_petri.png')
  },
  error=function(cond) {
    message(paste0("error building plot: inductive miner petri net."))
    print(paste0("error building plot: inductive miner petri net."))
  }
) 

```

```{r, echo = FALSE, warning = FALSE, out.width = "2920px",  out.height = "680px", fig.cap="Petri net with activities for decision mining"}
#| label: fig-decision_mining_petri_act

tryCatch(
  {
  include_graphics('../../outputs/decision_mining_petri_act.png')
  },
  error=function(cond) {
    message(paste0("error building plot: inductive miner petri net with activities."))
    print(paste0("error building plot: inductive miner petri net with activities."))
  }
) 

```



After that we created the petri net, we were able to see the importance of the features at decision/s point/s.

In this case, the point/s is/are: 

```{python}
#| label: print decision point
try:
    print('The decision point for fibrinolysis in hospital is: '+ point_decision[0])
    print('The decision point for thrombectomy in hospital is: '+ point_decision[1])
    print('The decision point for thrombolysis in emergency is: '+ point_decision[2])
except ValueError:
    logging.basicConfig(level=logging.INFO)
    logging.info('Error in function: Get decision mining points, so it is not possible to print decision point')

```

If the calculation of the importance of the variables in the decision does not appear for any point, it is because the complete decision mining process has not been carried out for that point due to lack of information complexity of the petri net.


```{r, echo = FALSE, results='asis',warning = FALSE}
#| label: fig-barplot_features_importance
list_files <- list.files(path = '../../outputs', pattern = 'barplot_features_importance_', full.names = TRUE)
cat("::: {.panel-tabset} \n")
for (i in 1:length(list_files)){
  cat("##### Decision point ", i, "\n")
  cat("\n")
  cat(paste0("![](",list_files[i],")"), "\n")
  #include_graphics(list_files[i])
  cat("\n")
}
cat(":::")


```



#### Prediction

The used method to make predictions is based on predicting the following activity, using the bupaR tool that makes use of a transformer model. 
Starting with predicting the next activity, the model's evaluation is: 

```{r, echo = FALSE, warning = FALSE, output = FALSE}
#| label: call predict_next_activity_function

source('aux_scripts/prediction_next_activity.R')

eval_compare_prediction <- predict_next_activity_function(params$data_path,n_epochs_fit=5)



```

```{r, echo = FALSE, warning = FALSE}
#| label: print eval model



round(eval_compare_prediction$eval,2)


```



A plot is also shown that allows a clear view of the accuracy of the model:
```{r, echo = FALSE, warning = FALSE, fig.cap="Confusion matrix for predictions"}
#| label: fig-roc_curve

y_df <- data.frame(y_train = eval_compare_prediction$hist_fit$metrics$sparse_categorical_accuracy,
                   y_val = eval_compare_prediction$hist_fit$metrics$val_sparse_categorical_accuracy)
y_df$epochs <- 1:nrow(y_df)

    
plot <- ggplot(data = y_df) + 
  geom_point(aes(x=epochs, y = y_train,text = paste("Epoch:", epochs, "\n Value: ",round(y_train,3))),color = "#377CE0" ,size=0.5) +
  geom_line(aes(x=epochs, y = y_train,color = 'Train')) +
  geom_point(aes(x=epochs, y = y_val,text = paste("Epoch:", epochs, "\n Value: ",round(y_val,3))), color = "#6C9E02" ,size=0.5) +
  geom_line(aes(x=epochs, y = y_val, color = 'Validation')) +
  labs(title = 'Performance model',
     y = 'Sparce categorical accuracy',
     x = 'Epoch') + scale_color_manual(name = "", values = c("Train" = "#377CE0", "Validation" = "#6C9E02")) + 
  theme(panel.background = element_blank(),legend.position = 'bottom', legend.text = element_text(size=10),
        axis.line = element_line(), legend.key=element_rect(fill="white"))
plot <- ggplotly(plot, tooltip = 'text') %>%
  layout(title = list(text = paste0('Performance model',
                                    '<br>',
                                    '<sup>')), legend = list(orientation = "h"))
plot
```


### Estimation of outcomes within a path

#### Kaplan-Meier survival plot
The kaplan-meier survival plot for the 4 intervention possibilities is shown below:


```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes

log_info('Estimation of outcomes analyses')
con = dbConnect(duckdb::duckdb(), dbdir=params$data_path, read_only=FALSE)
  
df <- dbGetQuery(conn = con, "SELECT a.* FROM main.patient_view a ")
df_ <- dbGetQuery(conn = con, "SELECT * FROM main.prediction_outcomes_db")
df_ <- df_[!duplicated(df_),]
df <- left_join(x=df,y=df_, by='patient_id')
rm(df_)

dbDisconnect(con, shutdown=TRUE)

#df <- read.csv(file = '../../outputs/df_gam.csv', sep = '|')
df <- df %>% mutate_if(is.logical, as.numeric)

df1 <- df %>% mutate(
  intervention = case_when(
    inhospital_thrombectomy_bl == 0 & inhospital_fibrinolysis_bl == 0 & thrombolysis_emergency_bl == 0 ~ 'none',
    inhospital_thrombectomy_bl == 0 & (inhospital_fibrinolysis_bl == 1 | thrombolysis_emergency_bl == 1) ~ 'fibrinolysis',
    inhospital_thrombectomy_bl == 1 & (inhospital_fibrinolysis_bl == 1 | thrombolysis_emergency_bl == 1) ~ 'combined',
    inhospital_thrombectomy_bl == 1 & inhospital_fibrinolysis_bl == 0 & thrombolysis_emergency_bl == 0 ~ 'thrombectomy_mec',
  )
)


start_study <- as.Date('2010-01-01')
end_study <-  as.Date('2022-12-31')

list_fest_spain <- as.Date(c('2010-01-01','2010-01-06','2010-04-02','2010-05-01','2010-10-12','2010-11-01','2010-12-06','2010-12-08','2010-12-25',
                             '2011-01-01','2011-01-06','2011-04-22','2011-08-15','2011-10-12','2011-11-01','2011-12-06','2011-12-08',
                             '2012-01-06','2012-04-06','2012-05-01','2012-08-15','2012-10-12','2012-11-01','2012-12-06','2012-12-08','2012-12-25',
                             '2013-01-01','2013-03-29','2013-05-01','2013-08-15','2013-10-12','2013-11-01','2013-12-06','2013-12-25',
                             '2014-01-01','2014-01-06','2014-04-17','2014-04-18','2014-05-01','2014-06-19','2014-08-15','2014-11-01','2014-12-06','2014-12-08','2014-12-25',
                             '2015-01-01','2015-01-06','2015-04-03','2015-05-01','2015-08-15','2015-10-12','2015-12-08','2015-12-25',
                             '2016-01-01','2016-01-06','2016-03-25','2016-08-15','2016-10-12','2016-12-06','2016-12-08',
                             '2017-01-06','2017-04-14','2017-05-01','2017-08-15','2017-10-12','2017-11-01','2017-12-06','2017-12-08','2017-12-25',
                             '2018-01-01','2018-03-30','2018-05-01','2018-08-15','2018-10-12','2018-11-01','2018-12-06','2018-12-08','2018-12-25',
                             '2019-01-01','2019-04-19','2019-05-01','2019-08-15','2019-10-12','2019-11-01','2019-12-06','2019-12-25',
                             '2020-01-01','2020-01-06','2020-04-10','2020-05-01','2020-08-15','2020-10-12','2020-12-08','2020-12-25',
                             '2021-01-01','2021-04-02','2021-05-01','2021-10-12','2021-11-01','2021-12-06','2021-12-08','2021-12-25',
                             '2022-01-01','2022-01-06','2022-04-15','2022-08-15','2022-10-12','2022-11-01','2022-12-06','2022-12-08',
                             '2023-04-07','2023-05-01','2023-08-15','2023-10-12','2023-11-01','2023-12-06','2023-12-08','2023-12-25'))

log_info('Create temporal variables')
df1 <- df1 %>% rowwise() %>% mutate(
  start_futime_dt = as.Date(min(admission_emergency_care_dt, hospital_admission_date_dt, na.rm=TRUE)),
  period = (12*(year(start_futime_dt)-year(start_study)) + month(start_futime_dt)),
  weekday = weekdays(as.POSIXlt(start_futime_dt),abbreviate = FALSE),
  weekend_bl = as.numeric(isHoliday(timeDate(start_futime_dt), holiday='ESP'))
)
df1$holiday_bl<-0
df1$holiday_bl[df1$start_futime_dt %in% list_fest_spain] <- 1
#### survival 12 months

df1 <- df1 %>% mutate(
  futime = case_when(
    !is.na(exitus_dt) ~ as.numeric((as.Date(exitus_dt)-start_futime_dt)),
    is.na(exitus_dt) ~ as.numeric(end_study - start_futime_dt)
  )
)
df1 <- df1 %>% mutate(
  exitus_1year_from_start_futimedt_bl= case_when(
    exitus_bl == 1 & futime <= 365 ~ 1,
    TRUE ~ 0
  )
)
#df1$futime[df1$futime > 365] <- 365

df1$intervention_bl[df1$intervention %in% 'none'] <- 0
df1$intervention_bl[df1$intervention %in% 'fibrinolysis'] <- 1
df1$intervention_bl[df1$intervention %in% 'thrombectomy_mec'] <- 1
df1$intervention_bl[df1$intervention %in% 'combined'] <- 1

df_model <- df1 %>% dplyr::select(patient_id,intervention,intervention_bl,hospital_cd, healthcare_area_cd , age_nm , sex_cd , municipality_code_cd , zip_code_cd , type_admission_cd , hospital_type_discharge_cd , ct_inhospital_bl , mri_inhospital_bl , n_admission_prior_emergency_nm , n_admission_prior_inhospital_nm , readmissions_30days_bl , antiarrhythmics_prescription_bl , antihypertensive_prescription_bl , antiaggregants_prescription_bl , fibrinolitics_prescriptions_bl ,  modified_rankin_scale_cd , barthel_index_nm , healthcare_area_cd , heart_failure_bl , hypertension_bl , diabetes_bl , atrial_fibrillation_bl , valvular_disease_bl , rank_trace , jaccard_similarity , dur_trace , period,exitus_bl,weekday, weekend_bl, holiday_bl,futime, exitus_1year_from_start_futimedt_bl)


df_model <- df_model %>% filter(modified_rankin_scale_cd <=5 | is.na(modified_rankin_scale_cd))
df_model <- df_model %>% filter(hospital_type_discharge_cd %nin% 4 | is.na(hospital_type_discharge_cd))

surv_obj <- Surv(time=df_model$futime,event=df_model$exitus_1year_from_start_futimedt_bl)

df_model$intervention <- factor(df_model$intervention, levels = c("none","fibrinolysis","thrombectomy_mec","combined"))

df_model$intervention = relevel(df_model$intervention, ref = "none")


surv_fit <- survfit(surv_obj ~ intervention, data = df_model)


ggsurvplot(surv_fit, data = df_model, xlab="Days of follow-up",
           ylab="Survival probability",
           xlim=c(0,365),
           ylim=c(0.6,1),
           break.x.by=50,
           conf.int = FALSE,
           main="Product-Limit Survival Estimates", risk.table = FALSE,
           title = "Survival analysis by intervention",
           legend.labs=c("None" ,"Fibrinolysis", "Thrombectomy", "Combined"))

```

#### General COX model

A summary (table with all variables and a ggforest with only variables that are statistically significant, *pvalue* < 0.01 (if it is possible)) of the constructed COX model is shown below:

```{r, echo = FALSE, warning=FALSE, fig.height=10, fig.width=10}
#| label: prediction outcomes cox model

df_model$sex_cd <- as.character(df_model$sex_cd)
#df_model$zip_code_cd <- as.character(df_model$zip_code_cd)
df_model$hospital_type_discharge_cd <- as.character(df_model$hospital_type_discharge_cd)

log_info('Build cox model')
# No hospital_cd in model

miss_threshold <- 0.2
df_model_aux <- df_model %>% select(intervention, age_nm, sex_cd, hospital_type_discharge_cd, ct_inhospital_bl, mri_inhospital_bl, n_admission_prior_emergency_nm, n_admission_prior_inhospital_nm,  antiarrhythmics_prescription_bl, antihypertensive_prescription_bl, antiaggregants_prescription_bl, fibrinolitics_prescriptions_bl,  modified_rankin_scale_cd,  heart_failure_bl, hypertension_bl, diabetes_bl, atrial_fibrillation_bl, valvular_disease_bl, rank_trace, jaccard_similarity, dur_trace, period, holiday_bl, weekend_bl, weekday)
a <- names(df_model_aux)[sapply(df_model_aux, function(x) (sum(is.na(x))/nrow(df_model_aux)) <= miss_threshold)]
df_model_aux <- df_model_aux %>% select(a)
formula_model <- paste0(colnames(df_model_aux), collapse = ' + ')
rm(df_model_aux)
formula_model <- as.formula(paste0('surv_obj ~ ',formula_model))

model_surv <- coxph(formula_model, data = df_model)

#survminer::ggforest(model_surv,data=as.data.frame(df_model))

df_model_ <- df_model %>% dplyr::select(intervention_bl, exitus_bl, hospital_cd , age_nm , sex_cd , zip_code_cd , hospital_type_discharge_cd , ct_inhospital_bl , mri_inhospital_bl , n_admission_prior_emergency_nm , n_admission_prior_inhospital_nm ,  antiarrhythmics_prescription_bl , antihypertensive_prescription_bl , antiaggregants_prescription_bl , fibrinolitics_prescriptions_bl ,  modified_rankin_scale_cd ,  heart_failure_bl , hypertension_bl , diabetes_bl , atrial_fibrillation_bl , valvular_disease_bl , rank_trace , jaccard_similarity , dur_trace , period , holiday_bl , weekend_bl , weekday, exitus_1year_from_start_futimedt_bl)
sum_model_surv <- summary(model_surv)
p <- as.data.frame(sum_model_surv$coefficients)
p1 <- p %>% filter(`Pr(>|z|)` <= 0.01)

aux1 <- data.frame(variables_df = colnames(df_model_))
aux2 <- data.frame(variables_model = rownames(p1))
aux3 <- data.frame(var=str_extract(aux2$variables_model,str_c(aux1$variables_df,collapse = '|')))
aux3 <- aux3 %>% filter(!is.na(var))

aux3 <- unique(aux3$var)

tryCatch(
  {


  tab_model(model_surv)
    
  },
  error=function(cond) {
    summary(model_surv)
  }
) 


tryCatch(
  {


  forest_model(model_surv, covariates = aux3)
    
  },
  error=function(cond) {
    message(paste0("error building forest plot: there are too many significant categorical variables to construct the plot."))
    print(paste0("error building forest plot: there are too many significant categorical variables to construct the plot."))
  }
) 



```


A test to see if proportional hazard assumption is satisfied, can be seen from the overall summary, *pvalue* < 0.05 indicates that proportional hazard assumption is not satisfied. If the test calculation is not possible, this could occur when there are collinear variables in the COX model, or when there are too few events.



```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes cox model test
tryCatch(
  {
log_info('Test cox model')
cox.zph(model_surv)
    
  },
  error=function(cond) {
    message(paste0("error calculating test."))
    print(paste0("error calculating test."))
  }
) 



#percent_cases <- 100*(1/exp(model_surv$coefficients)-1) # percent more

formula_model <- paste0(aux3,collapse = '+')
rm(aux1,aux2,aux3,p,p1)
#### Propension to intervention model


df_model_$intervention_fibrinolysis_bl <- if_else(df_model$intervention == 'fibrinolysis',1,0)
df_model_$intervention_thrombectomy_mec_bl <- if_else(df_model$intervention == 'thrombectomy_mec',1,0)
df_model_$intervention_combined_bl <- if_else(df_model$intervention == 'combined',1,0)

df_model_ <- df_model_ %>% rename(`fibrinolysis intervention bl` = intervention_fibrinolysis_bl, `thrombectomy mechanic intervention bl` = intervention_thrombectomy_mec_bl, `combined intervention bl` = intervention_combined_bl, `exitus bl` = exitus_1year_from_start_futimedt_bl)
```


#### Propension to intervention model



```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes propensity_global

### porpensity_global 
tryCatch(
  {
    formula_propensity_model_global <- as.formula(paste0('intervention_bl ~ hospital_cd +',formula_model))
    propensity_model_global <- glm(formula_propensity_model_global, data = df_model_, family = binomial)
    df_model_$probabilities_global <- propensity_model_global %>% predict(df_model_, type = "response")
    
    df_model_ <- df_model_ %>% mutate(ps_global = 1/(1-probabilities_global))
    
    
  },
  error=function(cond) {
    message(paste0("error building model: propension to intervention (global)"))
    print(paste0("error building model: propension to intervention (global)"))
  }
) 
```


The summary is displayed propensity to fibrinolysis intervention model:

```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes propensity_fibrinolisys
### propensity_fibrinolisys


tryCatch(
  {
    formula_propensity_model_fibri <- as.formula(paste0('`fibrinolysis intervention bl` ~ hospital_cd + ',formula_model))
    
    propensity_model_fibri <- glm(formula_propensity_model_fibri, data = df_model_, family = binomial)
    df_model_$probabilities_fibrinolysis <- propensity_model_fibri %>% predict(df_model_, type = "response")
    df_model_ <- df_model_ %>% mutate(ps_fibrinolysis = 1/(1-probabilities_fibrinolysis))
    tab_model(propensity_model_fibri)

    
  },
  error=function(cond) {
    message(paste0("error building model: propension to fibrinolisys intervention"))
    print(paste0("error building model: propension to fibrinolisys intervention"))
  }
)

```

The summary is displayed propensity to thrombectomy mechanic intervention model:


```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes propensity_thrombectomy_mec
### propensity_thrombectomy_mec


tryCatch(
  {
    formula_propensity_thrombectomy_mec <- as.formula(paste0('`thrombectomy mechanic intervention bl` ~ hospital_cd + ',formula_model))
    
    propensity_model_throm_mec <- glm(formula_propensity_thrombectomy_mec, data = df_model_, family = binomial)
    df_model_$probabilities_thrombectomy_mec <- propensity_model_throm_mec %>% predict(df_model_, type = "response")
    df_model_ <- df_model_ %>% mutate(ps_thrombectomy_mec = 1/(1-probabilities_thrombectomy_mec))
    tab_model(propensity_model_throm_mec)

    
  },
  error=function(cond) {
    message(paste0("error building model: propension to thrombectomy mechanic intervention"))
    print(paste0("error building model: propension to thrombectomy mechanic intervention"))
  }
)

```

The summary is shown propensity to combined intervention model:


```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes propensity_combined
### propensity_combined

tryCatch(
  {
    formula_propensity_combined <- as.formula(paste0('`combined intervention bl` ~ hospital_cd + ',formula_model))
    
    propensity_model_combined <- glm(formula_propensity_combined, data = df_model_, family = binomial)
    df_model_$probabilities_combined <- propensity_model_combined %>% predict(df_model_, type = "response")
    df_model_ <- df_model_ %>% mutate(ps_combined = 1/(1-probabilities_combined))
    tab_model(propensity_model_combined)

    
  },
  error=function(cond) {
    message(paste0("error building model: propension to combined intervention"))
    print(paste0("error building model: propension to combined intervention"))
  }
)

```

#### Model to predict exitus with PS as covariable



```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes ps_fibri model



tryCatch(
  {
    model_fibrinolysis <- glm(`exitus bl` ~ ps_fibrinolysis, data=df_model_, family = binomial)
    tab_model(model_fibrinolysis)
    
  },
  error=function(cond) {
    message(paste0("error building model: propensity score fibrinolysis as covariable"))
    print((paste0("error building model: propensity score fibrinolysis as covariable")))
  }
)

```

```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes ps_thromb model



tryCatch(
  {
    model_thrombectomy_mec <- glm(`exitus bl` ~ ps_thrombectomy_mec, data=df_model_, family = binomial)
    tab_model(model_thrombectomy_mec)
    
  },
  error=function(cond) {
    message(paste0("error building model: propensity score thrombectomy mechanic as covariable"))
    print(paste0("error building model: propensity score thrombectomy mechanic as covariable"))
  }
)

```

```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes ps_comb model



tryCatch(
  {
    model_combined <- glm(`exitus bl` ~ ps_combined, data=df_model_, family = binomial)
    tab_model(model_combined)
    
  },
  error=function(cond) {
    message(paste0("error building model: propensity score combined as covariable"))
    print(paste0("error building model: propensity score combined as covariable"))
  }
)

```





Finally, a model is built to predict exitus with PS calculated in each of the previous models as covariable and PS any interaction as offset:


```{r, echo = FALSE, warning=FALSE}
#| label: prediction outcomes full model
### full model


tryCatch(
  {
    model_full <- glm(`exitus bl` ~ ps_fibrinolysis + ps_thrombectomy_mec + ps_combined, data = df_model_, family = binomial, offset = ps_global)
    
    tab_model(model_full)
    
  },
  error=function(cond) {
    message(paste0("error building model: final model (propensity score as covariable)"))
    print(paste0("error building model: final model (propensity score as covariable)"))
  }
)

log_info('Analyses finish')
```



